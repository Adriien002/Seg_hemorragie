{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9503298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dossier ajouté au path : /store/home/tibia/Projet_Hemorragie/Seg_hemorragie/Multitask_model\n",
      "Tu peux maintenant importer config, data, et models.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# 1. Récupère le chemin absolu du dossier courant du notebook\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# 2. Récupère le chemin du dossier parent (le dossier 'Segmentation')\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "\n",
    "# 3. Ajoute ce dossier parent au path de Python\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "print(f\"Dossier ajouté au path : {parent_dir}\")\n",
    "print(\"Tu peux maintenant importer config, data, et models.\")\n",
    "\n",
    "\n",
    "import config\n",
    "import data.dataset as dataset\n",
    "from models.lightning_module  import MultiTaskHemorrhageModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc98e129",
   "metadata": {},
   "outputs": [],
   "source": [
    "import monai.transforms as T\n",
    "def get_val_transforms():\n",
    "    return T.Compose([\n",
    "        T.LoadImaged(keys=[\"image\", \"seg\"]),\n",
    "        T.EnsureChannelFirstd(keys=[\"image\", \"seg\"]),\n",
    "        T.CropForegroundd(keys=['image', 'seg'], source_key='image'),\n",
    "        T.Orientationd(keys=[\"image\", \"seg\"], axcodes='RAS'),\n",
    "        T.Spacingd(keys=[\"image\", \"seg\"],\n",
    "                   pixdim=(1., 1., 1.),\n",
    "                   mode=['bilinear', 'nearest']),\n",
    "        T.SpatialPadd(keys=[\"image\", \"seg\"],\n",
    "                      spatial_size=(96, 96, 96),),\n",
    "        T.ScaleIntensityRanged(keys=[\"image\"],\n",
    "                               a_min=-10, a_max=140,\n",
    "                               b_min=0.0, b_max=1.0,\n",
    "                               clip=True),\n",
    "    ])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f509153b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Démarrage de l'inférence UNIQUE (Images + Métriques)\n",
      "Chargement : /home/tibia/Projet_Hemorragie/MBH_multitask_64x64/best_model.ckpt\n",
      "BasicUNet features: (32, 32, 64, 128, 256, 32).\n",
      "Répartition des poids : tensor([1., 1., 1., 1., 1., 1.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tibia/Projet_Hemorragie/hemorragie-env/lib/python3.12/site-packages/monai/utils/deprecate_utils.py:321: FutureWarning: monai.transforms.croppad.dictionary CropForegroundd.__init__:allow_smaller: Current default value of argument `allow_smaller=True` has been deprecated since version 1.2. It will be changed to `allow_smaller=False` in version 1.5.\n",
      "  warn_deprecated(argname, msg, warning_category)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " shape image test : torch.Size([512, 512, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/tibia/Projet_Hemorragie/hemorragie-env/lib/python3.12/site-packages/monai/transforms/transform.py\", line 141, in apply_transform\n    return _apply_transform(transform, data, unpack_items, lazy, overrides, log_stats)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/tibia/Projet_Hemorragie/hemorragie-env/lib/python3.12/site-packages/monai/transforms/transform.py\", line 98, in _apply_transform\n    return transform(data, lazy=lazy) if isinstance(transform, LazyTrait) else transform(data)\n                                                                               ^^^^^^^^^^^^^^^\n  File \"/home/tibia/Projet_Hemorragie/hemorragie-env/lib/python3.12/site-packages/monai/transforms/io/dictionary.py\", line 162, in __call__\n    for key, meta_key, meta_key_postfix in self.key_iterator(d, self.meta_keys, self.meta_key_postfix):\n                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/tibia/Projet_Hemorragie/hemorragie-env/lib/python3.12/site-packages/monai/transforms/transform.py\", line 475, in key_iterator\n    raise KeyError(\nKeyError: 'Key `seg` of transform `LoadImaged` was missing in the data and allow_missing_keys==False.'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/tibia/Projet_Hemorragie/hemorragie-env/lib/python3.12/site-packages/torch/utils/data/_utils/worker.py\", line 351, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/tibia/Projet_Hemorragie/hemorragie-env/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"/home/tibia/Projet_Hemorragie/hemorragie-env/lib/python3.12/site-packages/monai/data/dataset.py\", line 108, in __getitem__\n    return self._transform(index)\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/tibia/Projet_Hemorragie/hemorragie-env/lib/python3.12/site-packages/monai/data/dataset.py\", line 412, in _transform\n    pre_random_item = self._cachecheck(self.data[index])\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/tibia/Projet_Hemorragie/hemorragie-env/lib/python3.12/site-packages/monai/data/dataset.py\", line 385, in _cachecheck\n    _item_transformed = self._pre_transform(deepcopy(item_transformed))  # keep the original hashed\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/tibia/Projet_Hemorragie/hemorragie-env/lib/python3.12/site-packages/monai/data/dataset.py\", line 323, in _pre_transform\n    item_transformed = self.transform(item_transformed, end=first_random, threading=True)\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/tibia/Projet_Hemorragie/hemorragie-env/lib/python3.12/site-packages/monai/transforms/compose.py\", line 335, in __call__\n    result = execute_compose(\n             ^^^^^^^^^^^^^^^^\n  File \"/home/tibia/Projet_Hemorragie/hemorragie-env/lib/python3.12/site-packages/monai/transforms/compose.py\", line 111, in execute_compose\n    data = apply_transform(\n           ^^^^^^^^^^^^^^^^\n  File \"/home/tibia/Projet_Hemorragie/hemorragie-env/lib/python3.12/site-packages/monai/transforms/transform.py\", line 171, in apply_transform\n    raise RuntimeError(f\"applying transform {transform}\") from e\nRuntimeError: applying transform <monai.transforms.io.dictionary.LoadImaged object at 0x7f9443c2cad0>\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 167\u001b[39m\n\u001b[32m    164\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m✅ CSV sauvegardé : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcsv_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 124\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    122\u001b[39m     \u001b[38;5;66;03m# 4. Boucle d'inférence\u001b[39;00m\n\u001b[32m    123\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[43m            \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mimage\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[38;5;66;43;03m#             # B. Inférence (Sliding Window)\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projet_Hemorragie/hemorragie-env/lib/python3.12/site-packages/tqdm/std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projet_Hemorragie/hemorragie-env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:701\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    698\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    699\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    700\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m701\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    702\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    703\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    704\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    705\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    706\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    707\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projet_Hemorragie/hemorragie-env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1465\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1463\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1464\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m._task_info[idx]\n\u001b[32m-> \u001b[39m\u001b[32m1465\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projet_Hemorragie/hemorragie-env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1491\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._process_data\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m   1489\u001b[39m \u001b[38;5;28mself\u001b[39m._try_put_index()\n\u001b[32m   1490\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[32m-> \u001b[39m\u001b[32m1491\u001b[39m     \u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1492\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projet_Hemorragie/hemorragie-env/lib/python3.12/site-packages/torch/_utils.py:715\u001b[39m, in \u001b[36mExceptionWrapper.reraise\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    711\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m    712\u001b[39m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[32m    713\u001b[39m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[32m    714\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m715\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[31mRuntimeError\u001b[39m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/tibia/Projet_Hemorragie/hemorragie-env/lib/python3.12/site-packages/monai/transforms/transform.py\", line 141, in apply_transform\n    return _apply_transform(transform, data, unpack_items, lazy, overrides, log_stats)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/tibia/Projet_Hemorragie/hemorragie-env/lib/python3.12/site-packages/monai/transforms/transform.py\", line 98, in _apply_transform\n    return transform(data, lazy=lazy) if isinstance(transform, LazyTrait) else transform(data)\n                                                                               ^^^^^^^^^^^^^^^\n  File \"/home/tibia/Projet_Hemorragie/hemorragie-env/lib/python3.12/site-packages/monai/transforms/io/dictionary.py\", line 162, in __call__\n    for key, meta_key, meta_key_postfix in self.key_iterator(d, self.meta_keys, self.meta_key_postfix):\n                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/tibia/Projet_Hemorragie/hemorragie-env/lib/python3.12/site-packages/monai/transforms/transform.py\", line 475, in key_iterator\n    raise KeyError(\nKeyError: 'Key `seg` of transform `LoadImaged` was missing in the data and allow_missing_keys==False.'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/tibia/Projet_Hemorragie/hemorragie-env/lib/python3.12/site-packages/torch/utils/data/_utils/worker.py\", line 351, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/tibia/Projet_Hemorragie/hemorragie-env/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"/home/tibia/Projet_Hemorragie/hemorragie-env/lib/python3.12/site-packages/monai/data/dataset.py\", line 108, in __getitem__\n    return self._transform(index)\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/tibia/Projet_Hemorragie/hemorragie-env/lib/python3.12/site-packages/monai/data/dataset.py\", line 412, in _transform\n    pre_random_item = self._cachecheck(self.data[index])\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/tibia/Projet_Hemorragie/hemorragie-env/lib/python3.12/site-packages/monai/data/dataset.py\", line 385, in _cachecheck\n    _item_transformed = self._pre_transform(deepcopy(item_transformed))  # keep the original hashed\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/tibia/Projet_Hemorragie/hemorragie-env/lib/python3.12/site-packages/monai/data/dataset.py\", line 323, in _pre_transform\n    item_transformed = self.transform(item_transformed, end=first_random, threading=True)\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/tibia/Projet_Hemorragie/hemorragie-env/lib/python3.12/site-packages/monai/transforms/compose.py\", line 335, in __call__\n    result = execute_compose(\n             ^^^^^^^^^^^^^^^^\n  File \"/home/tibia/Projet_Hemorragie/hemorragie-env/lib/python3.12/site-packages/monai/transforms/compose.py\", line 111, in execute_compose\n    data = apply_transform(\n           ^^^^^^^^^^^^^^^^\n  File \"/home/tibia/Projet_Hemorragie/hemorragie-env/lib/python3.12/site-packages/monai/transforms/transform.py\", line 171, in apply_transform\n    raise RuntimeError(f\"applying transform {transform}\") from e\nRuntimeError: applying transform <monai.transforms.io.dictionary.LoadImaged object at 0x7f9443c2cad0>\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from pathlib import Path\n",
    "from monai.data import DataLoader, PersistentDataset\n",
    "from tqdm import tqdm\n",
    "import monai.transforms as T\n",
    "import config \n",
    "import data.dataset as dataset\n",
    "import data.transform as T_seg\n",
    "from models.lightning_module import MultiTaskHemorrhageModule\n",
    "from monai.data import decollate_batch\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.metrics import DiceMetric, DiceHelper\n",
    "\n",
    "\n",
    "import monai.transforms as T\n",
    "def get_val_transforms():\n",
    "    return T.Compose([\n",
    "        T.LoadImaged(keys=[\"image\", \"label\"]),\n",
    "        T.EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
    "        T.CropForegroundd(keys=['image', 'label'], source_key='image'),\n",
    "        T.Orientationd(keys=[\"image\", \"label\"], axcodes='RAS'),\n",
    "        T.Spacingd(keys=[\"image\", \"label\"],\n",
    "                   pixdim=(1., 1., 1.),\n",
    "                   mode=['bilinear', 'nearest']),\n",
    "        T.SpatialPadd(keys=[\"image\", \"label\"],\n",
    "                      spatial_size=(96, 96, 96),),\n",
    "        T.ScaleIntensityRanged(keys=[\"image\"],\n",
    "                               a_min=-10, a_max=140,\n",
    "                               b_min=0.0, b_max=1.0,\n",
    "                               clip=True),\n",
    "    ])\n",
    "\n",
    "\n",
    "\n",
    "def save_nifti(data, affine, filename, output_dir):\n",
    "\n",
    "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    if data.ndim == 4:\n",
    "        data = np.transpose(data, (1, 2, 3, 0)) # (C, D, H, W) -> (D, H, W, C)\n",
    "        \n",
    "    nifti_img = nib.Nifti1Image(data, affine)\n",
    "    nib.save(nifti_img, os.path.join(output_dir, filename))\n",
    "    \n",
    "def main():\n",
    "  \n",
    "    pl.seed_everything(42)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # 1. Configuration\n",
    "    ckpt_path = Path(\"/home/tibia/Projet_Hemorragie/MBH_multitask_64x64/best_model.ckpt\")\n",
    "    output_seg_dir = Path(\"/home/tibia/Projet_Hemorragie/MBH_multitask_64x64/predictions_nifti\")\n",
    "    output_seg_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(f\" Démarrage de l'inférence UNIQUE (Images + Métriques)\")\n",
    "    print(f\"Chargement : {ckpt_path}\")\n",
    "\n",
    "    # 2. Modèle\n",
    "    model = MultiTaskHemorrhageModule.load_from_checkpoint(ckpt_path, num_steps=1000)\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    dice_helper = DiceHelper(\n",
    "    include_background=False,\n",
    "    softmax=True,\n",
    "    reduction=\"none\",\n",
    ")\n",
    "    val_transforms = get_val_transforms()\n",
    "     \n",
    "    def seg_predictor(x):\n",
    "        seg_logits, _ = model(x, task=\"segmentation\")\n",
    "        return seg_logits\n",
    "        # 3. Données (Test)\n",
    "        \n",
    "    test_files =dataset.get_segmentation_data(split=\"test\")\n",
    "    \n",
    "    test_dataset = PersistentDataset(\n",
    "        test_files,\n",
    "        transform=val_transforms, #meme transfo que val\n",
    "        cache_dir=os.path.join(output_seg_dir.parent, \"cache_test_inference\")\n",
    "    )\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=4)\n",
    "     \n",
    "     \n",
    "    # Test rapide : afficher la shape d'une image avant les transfos\n",
    "    load= T.LoadImageD(keys=['image']) \n",
    "    test_img = load( test_files[-1])  # pour afficher les infos de l'image\n",
    "    print (f\" shape image test : {test_img['image'].shape}\")\n",
    "    # print(f\"keys test img : {list(test_img.keys())}\")\n",
    "    # print(f\"Métadonnées de l'image : {test_img['image'].meta.keys()}\")\n",
    "    \n",
    "   \n",
    "    \n",
    "    #print du datalaoder\n",
    "    \n",
    "    # print(f\"Démarrage du DataLoader pour test...\")\n",
    "    # for i, data in enumerate(test_loader):\n",
    "    #     print(f\" Batch {i} :\")\n",
    "    #     for key in data.keys():\n",
    "    #         print(f\"  - {key} : type {type(data[key])}, shape {data[key].shape if isinstance(data[key], torch.Tensor) else 'N/A'}\")\n",
    "    #         print(f\"    Métadonnées ({key}.meta) : {data[key].meta.keys() if hasattr(data[key], 'meta') else 'N/A'}\")\n",
    "    #     if i >= 2:  # Limite à 3 batches pour éviter trop d'output\n",
    "    #         break\n",
    "\n",
    "   \n",
    "#     # 2. TRANSFORM & INVERTER\n",
    "   \n",
    "    \n",
    "#     # L'inverter a besoin de savoir quelles transfos inverser\n",
    "    inverter = T.Invertd(\n",
    "        keys=[\"pred\"],\n",
    "        transform=val_transforms,\n",
    "        orig_keys=[\"image\"],\n",
    "        nearest_interp=False, # On garde les probas fluides pour l'inversion\n",
    "        to_tensor=True\n",
    "    )\n",
    "    \n",
    "    results = [] # Pour stocker dice_scores\n",
    "    # 4. Boucle d'inférence\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader):\n",
    "            \n",
    "            images = batch[\"image\"].to(device)\n",
    "            \n",
    "#             # B. Inférence (Sliding Window)\n",
    "            batch[\"pred\"] = sliding_window_inference(images, roi_size=(64, 64, 64),  sw_batch_size=2,  predictor=seg_predictor, overlap=0.5,mode=\"gaussian\")\n",
    "            scores, _ = dice_helper(batch[\"pred\"], batch[\"seg\"].to(device)) # _ car get_non_nans =true  dans le dicehelper\n",
    "            print (f\" scores shape : {scores.shape}\")\n",
    "            dice_scores = {}\n",
    "            #for class_idx in range(3):  # In_house\n",
    "            for class_idx in range(5):  # MBH\n",
    "                dice_scores[f\"dice_c{class_idx+1}\"] = scores[0, class_idx].item()\n",
    "                \n",
    "        \n",
    "            \n",
    "            print (f\" shape pred avant inversion : {batch['pred'].shape}\")\n",
    "           \n",
    "            decollated_batch = decollate_batch(batch)\n",
    "            pred_inversed = inverter(decollated_batch[0]) #va l'appliquer automatiquement à la bonne clé \n",
    "            \n",
    "            pred = pred_inversed[\"pred\"]\n",
    "            print (f\" shape pred inversed : {pred.shape}\") \n",
    "            argmax_map = torch.argmax(pred, dim=0).cpu().numpy().astype(np.uint8)\n",
    "            affine = pred.meta[\"affine\"]\n",
    "            \n",
    "            filename =Path(decollated_batch[0][\"image\"].meta[\"filename_or_obj\"]).name\n",
    "          \n",
    "\n",
    "            row = {\n",
    "                \"filename\": filename,\n",
    "            }\n",
    "            row.update(dice_scores)\n",
    "\n",
    "            results.append(row)\n",
    "            save_nifti(argmax_map, affine, f\"PRED_{filename}\", output_seg_dir)\n",
    "            df = pd.DataFrame(results)\n",
    "     # 5. Sauvegarde des résultats dans un CSV pour inférence per patient       \n",
    "    csv_path = os.path.join(output_seg_dir.parent, \"inference_metrics.csv\")\n",
    "    df.to_csv(csv_path, index=False)\n",
    "\n",
    "    print(f\"✅ CSV sauvegardé : {csv_path}\")\n",
    "   \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "            \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hemorragie-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
