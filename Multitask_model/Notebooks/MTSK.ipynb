{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tibia/Projet_Hemorragie/hemorragie-env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Chemin du dossier parent\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "import data.dataset as dataset\n",
    "import data.transform as T_mtsk\n",
    "\n",
    "# Import depuis models/\n",
    "from models.architecture import BasicUNetWithClassification\n",
    "from models.lightning_module import MultiTaskHemorrhageModule, MultiTaskHemorrhageModule_homeo, MultiTaskHemorrhageModule_gradnorm, MultiTaskSoftSharing\n",
    "\n",
    "import utils\n",
    "\n",
    "from monai.data import DataLoader, PersistentDataset\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import torch\n",
    "import os\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "import config\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN - Positifs: 371 | Négatifs: 371\n",
      "Nombre de données de segmentation : 154\n",
      "Nombre de données de classification équilibrées : 742\n",
      "VAL - Positifs: 92 | Négatifs: 92\n",
      "Nombre de données de segmentation : 38\n",
      "Nombre de données de classification équilibrées : 184\n",
      "Nombre d'images d'entraînement : 896\n",
      "Nombre d'images de validation : 222\n",
      "Exemple d'entrée de train_data : {'image': '/home/tibia/Projet_Hemorragie/MBH_label_case/ID_dfcea57b_ID_331f2c76e6.nii.gz', 'label': array([1., 0., 0., 0., 1., 0.], dtype=float32), 'task': 'classification'}\n",
      "visualisation de quel taches sont présentes dans les données d'entraînement :\n",
      "  classification\n",
      "  segmentation\n",
      "  classification\n",
      "  segmentation\n",
      "  classification\n",
      "  classification\n",
      "  classification\n",
      "  classification\n",
      "  classification\n",
      "  classification\n",
      "=== Test direct du dataset ===\n",
      "\n",
      "Item 0:\n",
      "  Task: classification\n",
      "  Image shape: no shape\n",
      "\n",
      "Item 1:\n",
      "  Task: segmentation\n",
      "  Image shape: no shape\n",
      "\n",
      "Item 2:\n",
      "  Task: classification\n",
      "  Image shape: no shape\n",
      "\n",
      "Item 3:\n",
      "  Task: segmentation\n",
      "  Image shape: no shape\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_data=dataset.get_equalized_multitask_dataset('train')\n",
    "random.shuffle(train_data)  # Mélange des données pour l'entraînement\n",
    "val_data=dataset.get_equalized_multitask_dataset('val')\n",
    "\n",
    "print(f\"Nombre d'images d'entraînement : {len(train_data)}\")\n",
    "print(f\"Nombre d'images de validation : {len(val_data)}\")\n",
    "\n",
    "print(\"Exemple d'entrée de train_data :\", train_data[0])\n",
    "\n",
    "print(\"visualisation de quel taches sont présentes dans les données d'entraînement :\")\n",
    "for item in train_data[:10]:\n",
    "    print(f\"  {item['task']}\")\n",
    "    \n",
    "print(\"=== Test direct du dataset ===\")\n",
    "for i in range(min(4, len(train_data))):\n",
    "    item = train_data[i]\n",
    "    print(f\"\\nItem {i}:\")\n",
    "    print(f\"  Task: {item.get('task')}\")\n",
    "    if 'image' in item:\n",
    "        if isinstance(item['image'], list):\n",
    "            print(f\"  Image: LIST de {len(item['image'])} éléments\")\n",
    "            print(f\"    Premier élément shape: {item['image'][0].shape if hasattr(item['image'][0], 'shape') else 'no shape'}\")\n",
    "        else:\n",
    "            print(f\"  Image shape: {item['image'].shape if hasattr(item['image'], 'shape') else 'no shape'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> TaskBasedTransform initialized\n",
      ">>> TaskBasedTransform initialized\n",
      "Échantillon 0 :\n",
      "  - image: /home/tibia/Projet_Hemorragie/MBH_label_case/ID_dfcea57b_ID_331f2c76e6.nii.gz\n",
      "  - label: [1. 0. 0. 0. 1. 0.]\n",
      "  - task: classification\n",
      "Échantillon 1 :\n",
      "  - image: /home/tibia/Projet_Hemorragie/Seg_hemorragie/split_MONAI/train/img/ID_a84354a5_ID_6699da85bb.nii.gz\n",
      "  - label: /home/tibia/Projet_Hemorragie/Seg_hemorragie/split_MONAI/train/seg/ID_a84354a5_ID_6699da85bb.nii.gz\n",
      "  - task: segmentation\n",
      "Échantillon 2 :\n",
      "  - image: /home/tibia/Projet_Hemorragie/MBH_label_case/ID_67b355ba_ID_52ff5ab9b6.nii.gz\n",
      "  - label: [0. 0. 0. 0. 0. 0.]\n",
      "  - task: classification\n",
      "Échantillon 3 :\n",
      "  - image: /home/tibia/Projet_Hemorragie/Seg_hemorragie/split_MONAI/train/img/ID_96967deb_ID_5d782e9379.nii.gz\n",
      "  - label: /home/tibia/Projet_Hemorragie/Seg_hemorragie/split_MONAI/train/seg/ID_96967deb_ID_5d782e9379.nii.gz\n",
      "  - task: segmentation\n",
      "Échantillon 4 :\n",
      "  - image: /home/tibia/Projet_Hemorragie/MBH_label_case/ID_3922d1cd_ID_b835bd35de.nii.gz\n",
      "  - label: [0. 0. 0. 0. 0. 0.]\n",
      "  - task: classification\n",
      "Échantillon 5 :\n",
      "  - image: /home/tibia/Projet_Hemorragie/MBH_label_case/ID_6387a24a_ID_5e67ee05b4.nii.gz\n",
      "  - label: [0. 0. 0. 0. 0. 0.]\n",
      "  - task: classification\n",
      "Échantillon 6 :\n",
      "  - image: /home/tibia/Projet_Hemorragie/MBH_label_case/ID_3c48ba20_ID_2e9e749dff.nii.gz\n",
      "  - label: [0. 0. 0. 0. 0. 0.]\n",
      "  - task: classification\n",
      "Échantillon 7 :\n",
      "  - image: /home/tibia/Projet_Hemorragie/MBH_label_case/ID_fa949caf_ID_914c8cea7a.nii.gz\n",
      "  - label: [1. 1. 0. 0. 1. 1.]\n",
      "  - task: classification\n",
      "Échantillon 8 :\n",
      "  - image: /home/tibia/Projet_Hemorragie/MBH_label_case/ID_fb982b0d_ID_5310996be7.nii.gz\n",
      "  - label: [1. 0. 1. 0. 0. 0.]\n",
      "  - task: classification\n",
      "Échantillon 9 :\n",
      "  - image: /home/tibia/Projet_Hemorragie/MBH_label_case/ID_5e035492_ID_cbd3dc5316.nii.gz\n",
      "  - label: [1. 0. 1. 0. 1. 0.]\n",
      "  - task: classification\n",
      "Échantillon 10 :\n",
      "  - image: /home/tibia/Projet_Hemorragie/Seg_hemorragie/split_MONAI/train/img/ID_d3d99dce_ID_cd10b6aba1.nii.gz\n",
      "  - label: /home/tibia/Projet_Hemorragie/Seg_hemorragie/split_MONAI/train/seg/ID_d3d99dce_ID_cd10b6aba1.nii.gz\n",
      "  - task: segmentation\n",
      "Échantillon 11 :\n",
      "  - image: /home/tibia/Projet_Hemorragie/MBH_label_case/ID_1e74903a_ID_20603762d2.nii.gz\n",
      "  - label: [1. 1. 0. 0. 0. 0.]\n",
      "  - task: classification\n",
      "Échantillon 12 :\n",
      "  - image: /home/tibia/Projet_Hemorragie/MBH_label_case/ID_bee11f07_ID_b19dc1e6b2.nii.gz\n",
      "  - label: [1. 0. 1. 0. 0. 1.]\n",
      "  - task: classification\n",
      "Échantillon 13 :\n",
      "  - image: /home/tibia/Projet_Hemorragie/MBH_label_case/ID_a6d9799e_ID_63a0fecadc.nii.gz\n",
      "  - label: [1. 1. 0. 0. 0. 1.]\n",
      "  - task: classification\n",
      "Échantillon 14 :\n",
      "  - image: /home/tibia/Projet_Hemorragie/MBH_label_case/ID_6b8b7acd_ID_4c2fd95224.nii.gz\n",
      "  - label: [1. 0. 1. 0. 1. 0.]\n",
      "  - task: classification\n",
      "Nombre total d'éléments dans le dataset: 896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tibia/Projet_Hemorragie/hemorragie-env/lib/python3.12/site-packages/monai/utils/deprecate_utils.py:321: FutureWarning: monai.transforms.croppad.dictionary CropForegroundd.__init__:allow_smaller: Current default value of argument `allow_smaller=True` has been deprecated since version 1.2. It will be changed to `allow_smaller=False` in version 1.5.\n",
      "  warn_deprecated(argname, msg, warning_category)\n"
     ]
    }
   ],
   "source": [
    "from monai.data import Dataset\n",
    "import data.transform as T_mtsk\n",
    "\n",
    "train_transforms, val_transforms = T_mtsk.TaskBasedTransform_V2(keys=[\"image\", \"label\"]), T_mtsk.TaskBasedValTransform_V2(keys=[\"image\", \"label\"])\n",
    "\n",
    "import shutil\n",
    "\n",
    "# Supprime complètement le cache\n",
    "shutil.rmtree(\"./cache_dir\", ignore_errors=True)\n",
    "\n",
    "train_dataset = PersistentDataset(\n",
    "        train_data, \n",
    "        transform=train_transforms,\n",
    "        cache_dir=\"./cache_dir\")\n",
    "\n",
    "\n",
    "for i in range(15):\n",
    "    print(f\"Échantillon {i} :\")\n",
    "    for key, value in train_data[i].items():\n",
    "         print(f\"  - {key}: {value}\")\n",
    "print(\"Nombre total d'éléments dans le dataset:\", len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset initial (volumes) : 896\n",
      "Batch Size (volumes) : 4\n",
      "--------------------------------------------------\n",
      "Vérification de la taille des lots APRÈS l'application des patches (via collate_fn) :\n",
      "Nombre total de lots dans le DataLoader : 224\n",
      "Résultat du premier lot :\n",
      "=== Infos générales ===\n",
      "type: <class 'dict'>\n",
      "keys: ['classification', 'segmentation']\n",
      "=== Infos par clé ===\n",
      "Patches de classification: 48\n",
      "Patches de segmentation: 0\n",
      "  shape : [48, 1, 96, 96, 96]\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=4, \n",
    "    shuffle=True, # Pour plus de clarté\n",
    "    num_workers=0,\n",
    "    collate_fn=utils.multitask_collate_fn # Utilisation de la fonction de collate personnalisée\n",
    "\n",
    ")\n",
    "\n",
    "# --- Exécution du Test ---\n",
    "print(f\"Dataset initial (volumes) : {len(train_data)}\")\n",
    "print(f\"Batch Size (volumes) : {loader.batch_size}\")\n",
    "print(\"-\" * 50)\n",
    "print(\"Vérification de la taille des lots APRÈS l'application des patches (via collate_fn) :\")\n",
    "\n",
    "print (f\"Nombre total de lots dans le DataLoader : {len(loader)}\")\n",
    "print(\"Résultat du premier lot :\")\n",
    "first_batch = next(iter(loader))\n",
    "\n",
    "# Avec le call initial \n",
    "\n",
    "print(\"=== Infos générales ===\")\n",
    "print(f\"type: {type(first_batch)}\")\n",
    "print(f\"keys: {list(first_batch.keys())}\")\n",
    "\n",
    "print(\"=== Infos par clé ===\")\n",
    "\n",
    "cls_count = first_batch['classification']['image'].shape[0] if first_batch['classification'] is not None else 0\n",
    "seg_count = first_batch['segmentation']['image'].shape[0] if first_batch['segmentation'] is not None else 0\n",
    "total_patches = cls_count + seg_count\n",
    "\n",
    "print(f\"Patches de classification: {cls_count}\")\n",
    "print(f\"Patches de segmentation: {seg_count}\")\n",
    "\n",
    "print(f\"  shape : {list(first_batch['classification'][\"image\"].shape)}\")\n",
    "\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Test Dataloader utils "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Structure du Batch (Avant flatten) ---\n",
      "Type de l'élément à l'index 1: <class 'list'>\n",
      "Longueur du batch: 4\n",
      "\n",
      "--- 2. Structure du Batch (Après flatten) ---\n",
      "Type du résultat: <class 'list'>\n",
      "Longueur du batch aplati: 5\n",
      "\n",
      "--- 3. Vérification des Tenseurs ---\n",
      "Le Tenseur A est-il le premier élément? True\n",
      "Le Tenseur E est-il le dernier élément? True\n",
      "Forme du premier élément aplati: torch.Size([3, 10])\n",
      "Type du dernier élément aplati: <class 'torch.Tensor'>\n",
      "batch aplati complet: [tensor([[ 0.8457,  0.4845,  1.5668, -1.8926, -0.6487,  1.3580,  0.9388,  1.2377,\n",
      "          1.1349, -0.7695],\n",
      "        [-1.1854,  0.1463,  1.2118, -1.5987, -0.0058, -1.6769,  0.1111,  0.9685,\n",
      "         -0.9121, -0.6517],\n",
      "        [-1.3885, -1.3750,  0.7516,  1.0253,  0.2023,  0.6105,  0.7807,  1.9234,\n",
      "         -0.6046,  1.6436]]), tensor([[-0.7419,  0.5092, -0.0504, -1.5096, -1.2095, -2.1121,  0.1571, -0.8586,\n",
      "         -0.2099, -0.7358],\n",
      "        [ 0.5918,  1.1614,  1.2502, -0.4430,  2.1776, -0.8310,  1.3217, -0.2493,\n",
      "          0.6168,  0.3143],\n",
      "        [-1.3261,  0.1065,  0.9568,  1.7908,  0.4614, -0.9758, -0.8161, -0.9475,\n",
      "         -0.2208, -0.7026]]), tensor([[-1.8496, -0.6738,  2.1856, -0.2418, -0.5983, -0.2874, -1.3509,  0.6313,\n",
      "          1.4191, -1.0960],\n",
      "        [ 0.7638,  0.3149, -1.1460, -0.0464,  1.7194,  0.2801, -0.8459,  0.4976,\n",
      "          0.3142,  0.3932],\n",
      "        [-2.5817,  0.3230, -0.3461, -1.6202,  1.3940, -0.7832,  0.2578, -0.8126,\n",
      "         -1.7013,  0.0949]]), tensor([[-0.1158, -0.4067, -0.0150, -0.0955,  0.8748,  0.2725, -0.3698, -0.8859,\n",
      "          1.1669,  0.1912],\n",
      "        [ 0.1955, -1.8041, -0.1270, -0.0527,  0.0489, -0.2688, -0.0691,  0.5009,\n",
      "          0.3749,  0.8170],\n",
      "        [-0.1164, -0.5574, -1.2977,  1.7552, -0.0643, -0.4665,  1.3012,  0.1143,\n",
      "         -1.5572,  1.6050]]), tensor([[-1.4585, -0.0215,  0.8029, -0.5155,  0.0480, -0.0265, -0.2745,  1.2175,\n",
      "         -1.6675, -0.5963],\n",
      "        [ 0.8520,  0.4857, -1.0240,  0.9254, -0.7015, -1.6633,  0.3524,  1.0640,\n",
      "         -1.3338,  0.5235],\n",
      "        [ 0.9669,  1.7447, -1.5167, -1.4900,  0.4177, -0.3121,  1.0478,  1.4551,\n",
      "         -1.3393, -0.0308]])]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Votre fonction flatten\n",
    "def flatten(batch):\n",
    "    for item in batch:\n",
    "        if isinstance(item, list):\n",
    "            yield from flatten(item)\n",
    "        else:\n",
    "            yield item\n",
    "\n",
    "# --- Création d'un batch d'exemple ---\n",
    "# Nous simulerons des données simples (tenseurs) et une imbrication\n",
    "# pour montrer que flatten fonctionne.\n",
    "\n",
    "# Tenseurs aléatoires\n",
    "tensor_A = torch.randn(3, 10)  # Tenseur de 3x10\n",
    "tensor_B = torch.randn(3, 10)  # Tenseur de 3x10\n",
    "tensor_C = torch.randn(3, 10)  # Tenseur de 3x10\n",
    "tensor_D = torch.randn(3, 10)  # Tenseur de 3x10\n",
    "tensor_E = torch.randn(3, 10)  # Tenseur de 3x10\n",
    "\n",
    "# Batch imbriqué\n",
    "# [ Élément simple, [ Liste d'éléments simples ], Élément simple, [ Liste imbriquée ] ]\n",
    "example_batch = [\n",
    "    tensor_A,\n",
    "    [tensor_B, tensor_C],\n",
    "    tensor_D,\n",
    "    [[tensor_E]] # Double imbrication\n",
    "]\n",
    "\n",
    "print(\"--- 1. Structure du Batch (Avant flatten) ---\")\n",
    "print(f\"Type de l'élément à l'index 1: {type(example_batch[1])}\")\n",
    "print(f\"Longueur du batch: {len(example_batch)}\")\n",
    "\n",
    "\n",
    "# --- Test de la fonction flatten ---\n",
    "flat_batch = list(flatten(example_batch))\n",
    "\n",
    "print(\"\\n--- 2. Structure du Batch (Après flatten) ---\")\n",
    "print(f\"Type du résultat: {type(flat_batch)}\")\n",
    "print(f\"Longueur du batch aplati: {len(flat_batch)}\")\n",
    "\n",
    "print(\"\\n--- 3. Vérification des Tenseurs ---\")\n",
    "\n",
    "# On vérifie que les tenseurs originaux sont tous présents\n",
    "print(f\"Le Tenseur A est-il le premier élément? {flat_batch[0] is tensor_A}\")\n",
    "print(f\"Le Tenseur E est-il le dernier élément? {flat_batch[-1] is tensor_E}\")\n",
    "print(f\"Forme du premier élément aplati: {flat_batch[0].shape}\")\n",
    "print(f\"Type du dernier élément aplati: {type(flat_batch[-1])}\")\n",
    "print(f\"batch aplati complet: {flat_batch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Répartition dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "csv_path = Path(\"/home/tibia/Projet_Hemorragie/MBH_label_case/case-wise_annotation.csv\")\n",
    "label_cols = ['any', 'epidural', 'intraparenchymal', 'intraventricular', 'subarachnoid', 'subdural']\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "def visualize_label_distribution(label_file_counts, total_files):\n",
    "    fig = plt.figure(figsize=(20, 12))\n",
    "    \n",
    "    labels = list(range(6))\n",
    "    counts = [label_file_counts[i] for i in labels]\n",
    "    label_names = ['Background', 'EDH', 'IPH', 'IVH', 'SAH', 'SDH']\n",
    "    \n",
    "    colors = ['#2C3E50', '#E74C3C', '#3498DB', '#F39C12', '#9B59B6', '#1ABC9C']\n",
    "    \n",
    "    ax1 = plt.subplot(2, 3, 1)\n",
    "    bars = ax1.bar(labels, counts, color=colors, alpha=0.8, edgecolor='black', linewidth=1)\n",
    "    ax1.set_xlabel('Label', fontsize=12, fontweight='bold')\n",
    "    ax1.set_ylabel('Number of Files', fontsize=12, fontweight='bold')\n",
    "    ax1.set_title('Files Containing Each Label', fontsize=14, fontweight='bold')\n",
    "    ax1.set_xticks(labels)\n",
    "    ax1.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    for bar, count in zip(bars, counts):\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
    "                f'{count}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    ax2 = plt.subplot(2, 3, 2)\n",
    "    percentages = [(count/total_files)*100 for count in counts]\n",
    "    bars2 = ax2.barh(label_names, percentages, color=colors, alpha=0.8, edgecolor='black', linewidth=1)\n",
    "    ax2.set_xlabel('Percentage of Files (%)', fontsize=12, fontweight='bold')\n",
    "    ax2.set_title('Percentage Distribution', fontsize=14, fontweight='bold')\n",
    "    ax2.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    for i, (bar, pct) in enumerate(zip(bars2, percentages)):\n",
    "        width = bar.get_width()\n",
    "        ax2.text(width + 1, bar.get_y() + bar.get_height()/2.,\n",
    "                f'{pct:.1f}%', ha='left', va='center', fontweight='bold')\n",
    "    \n",
    "    ax3 = plt.subplot(2, 3, 3)\n",
    "    hemorrhage_counts = counts[1:]\n",
    "    hemorrhage_labels = label_names[1:]\n",
    "    hemorrhage_colors = colors[1:]\n",
    "    \n",
    "    wedges, texts, autotexts = ax3.pie(hemorrhage_counts, labels=hemorrhage_labels, \n",
    "                                       colors=hemorrhage_colors, autopct='%1.1f%%',\n",
    "                                       startangle=90, explode=(0.05, 0.05, 0.05, 0.05, 0.05))\n",
    "    ax3.set_title('Hemorrhage Types Distribution\\n(Excluding Background)', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    for autotext in autotexts:\n",
    "        autotext.set_color('white')\n",
    "        autotext.set_fontweight('bold')\n",
    "    \n",
    "    ax4 = plt.subplot(2, 3, 4)\n",
    "    background_count = counts[0]\n",
    "    lesion_counts = counts[1:]\n",
    "    \n",
    "    bottom = 0\n",
    "    bar_width = 0.6\n",
    "    \n",
    "    ax4.bar('Dataset', background_count, bar_width, label='Background', \n",
    "            color=colors[0], alpha=0.8, edgecolor='black', linewidth=1)\n",
    "    bottom += background_count\n",
    "    \n",
    "    for i, (count, color, name) in enumerate(zip(lesion_counts, colors[1:], label_names[1:])):\n",
    "        ax4.bar('Dataset', count, bar_width, bottom=bottom, label=name,\n",
    "                color=color, alpha=0.8, edgecolor='black', linewidth=1)\n",
    "        bottom += count\n",
    "    \n",
    "    ax4.set_ylabel('Number of Files', fontsize=12, fontweight='bold')\n",
    "    ax4.set_title('Stacked Distribution', fontsize=14, fontweight='bold')\n",
    "    ax4.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    ax5 = plt.subplot(2, 3, 5)\n",
    "    wedges, texts = ax5.pie(counts, labels=labels, colors=colors, \n",
    "                           wedgeprops=dict(width=0.5), startangle=90)\n",
    "    \n",
    "    ax5.text(0, 0, f'Total\\n{total_files}\\nFiles', ha='center', va='center',\n",
    "             fontsize=14, fontweight='bold')\n",
    "    ax5.set_title('Complete Distribution\\n(Donut Chart)', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    ax6 = plt.subplot(2, 3, 6)\n",
    "    ax6.axis('off')\n",
    "    \n",
    "    summary_data = []\n",
    "    for i, (label, count, name) in enumerate(zip(labels, counts, label_names)):\n",
    "        pct = (count/total_files)*100\n",
    "        summary_data.append([f'Label {label}', name, count, f'{pct:.1f}%'])\n",
    "    \n",
    "    summary_data.append(['', 'TOTAL FILES', total_files, '100.0%'])\n",
    "    summary_data.append(['', 'Files w/ Lesions', (df['any'] == 1).sum(), ''])\n",
    "    \n",
    "    table = ax6.table(cellText=summary_data,\n",
    "                     colLabels=['Label', 'Type', 'Count', 'Percentage'],\n",
    "                     cellLoc='center',\n",
    "                     loc='center',\n",
    "                     colWidths=[0.15, 0.45, 0.2, 0.2])\n",
    "    \n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(10)\n",
    "    table.scale(1, 2)\n",
    "    \n",
    "    for i in range(len(summary_data) + 1):\n",
    "        for j in range(4):\n",
    "            cell = table[(i, j)]\n",
    "            if i == 0:\n",
    "                cell.set_facecolor('#34495E')\n",
    "                cell.set_text_props(weight='bold', color='white')\n",
    "            elif i == len(summary_data) - 1 or i == len(summary_data):\n",
    "                cell.set_facecolor('#ECF0F1')\n",
    "                cell.set_text_props(weight='bold')\n",
    "            else:\n",
    "                cell.set_facecolor(colors[i-1] if i-1 < len(colors) else '#FFFFFF')\n",
    "                if i-1 == 0:\n",
    "                    cell.set_text_props(color='white', weight='bold')\n",
    "    \n",
    "    ax6.set_title('Summary Statistics', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    plt.suptitle('Hemorrhage Dataset Annotation Distribution Analysis', \n",
    "                 fontsize=18, fontweight='bold', y=0.98)\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"SUMMARY STATISTICS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Total files: {total_files}\")\n",
    "    print(f\"Files with background (Label 0): {counts[0]} ({counts[0]/total_files*100:.1f}%)\")\n",
    "    print(\"Files with hemorrhage types:\")\n",
    "    for i in range(1, 6):\n",
    "        pct = (counts[i]/total_files)*100\n",
    "        print(f\"  Label {i}: {counts[i]} files ({pct:.1f}%)\")\n",
    "    \n",
    "    total_lesion_files_present = (df['any'] == 1).sum()\n",
    "    print(f\"\\nFiles with at least one hemorrhage (any=1): {total_lesion_files_present}\")\n",
    "\n",
    "total_files = len(df)\n",
    "label_file_counts = [0] * 6\n",
    "label_file_counts[0] = (df['any'] == 0).sum() # Background (no hemorrhage)\n",
    "label_file_counts[1] = (df['epidural'] == 1).sum()\n",
    "label_file_counts[2] = (df['intraparenchymal'] == 1).sum()\n",
    "label_file_counts[3] = (df['intraventricular'] == 1).sum()\n",
    "label_file_counts[4] = (df['subarachnoid'] == 1).sum()\n",
    "label_file_counts[5] = (df['subdural'] == 1).sum()\n",
    "\n",
    "visualize_label_distribution(label_file_counts, total_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deboggage avec pprint"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hemorragie-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
